{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bd88c0-224d-49ac-961f-7c688adf02e8",
   "metadata": {},
   "source": [
    "## 6-a. Embedding 계획\n",
    "- 1. 토큰을 벡터화한다\n",
    "  - 대표적으로 'Bag of Words', 'TF-IDF', 'Word2Vec' 의 방법이 있다\n",
    "  - 여기서는 'TF-IDF' 를 사용해보려고 한다.\n",
    "- 2. 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19e801-9bcf-4fa8-bc56-4630c0ea516f",
   "metadata": {},
   "source": [
    "## 6-b. 임베딩 진행 : TF-IDF 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd09cfb-b825-4cb6-9c14-5d992be70902",
   "metadata": {},
   "source": [
    "### 6-b-1. 파일 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fbc0663-1a8b-4fad-beaf-28acf146e84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_keyword</th>\n",
       "      <th>date_created</th>\n",
       "      <th>time_created</th>\n",
       "      <th>writer</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>id</th>\n",
       "      <th>spaced_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_reduced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891011</th>\n",
       "      <td>휴젤</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>11:55:10</td>\n",
       "      <td>ㅇㅇ(123.215)</td>\n",
       "      <td>0</td>\n",
       "      <td>241400125127</td>\n",
       "      <td>미국주식보다 한국꺼보면 ㄹㅇ 암걸릴 것 같음 특히 휴젤 저거 아는분이 임원진인데 회...</td>\n",
       "      <td>['미국', '주식', '보다', '한국', '꺼', '보', '면', 'ㄹㅇ', ...</td>\n",
       "      <td>['미국', '주식', '보다', '한국', '꺼', '보', '면', 'ㄹㅇ', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891012</th>\n",
       "      <td>휴젤</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>11:56:16</td>\n",
       "      <td>ㅇㄷ(175.223)</td>\n",
       "      <td>1</td>\n",
       "      <td>241400125127</td>\n",
       "      <td>한국 상장회사들중에 ㅂㅅ같은곳이 넘 많음...</td>\n",
       "      <td>['한국', '상장', '회사', '들', '중', '에', 'ㅂㅅ', '같', '...</td>\n",
       "      <td>['한국', '상장', '회사', '들', '중', '에', 'ㅂㅅ', '같', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891013</th>\n",
       "      <td>휴젤</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>11:56:38</td>\n",
       "      <td>ㅇㅇ(223.33)</td>\n",
       "      <td>1</td>\n",
       "      <td>241400125127</td>\n",
       "      <td>신흥국 회사투자 망설여지는 게 이거 때문임. 주식시장에 대한 개념이 안 잡혀있음.</td>\n",
       "      <td>['신흥국', '회사', '투자', '망설여', '지', '는', '게', '이거'...</td>\n",
       "      <td>['신흥국', '회사', '투자', '&lt;UNK&gt;', '지', '는', '게', '이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       search_keyword date_created time_created       writer  is_reply  \\\n",
       "891011             휴젤   2020-04-29     11:55:10  ㅇㅇ(123.215)         0   \n",
       "891012             휴젤   2020-04-29     11:56:16  ㅇㄷ(175.223)         1   \n",
       "891013             휴젤   2020-04-29     11:56:38   ㅇㅇ(223.33)         1   \n",
       "\n",
       "                  id                                        spaced_text  \\\n",
       "891011  241400125127  미국주식보다 한국꺼보면 ㄹㅇ 암걸릴 것 같음 특히 휴젤 저거 아는분이 임원진인데 회...   \n",
       "891012  241400125127                          한국 상장회사들중에 ㅂㅅ같은곳이 넘 많음...   \n",
       "891013  241400125127      신흥국 회사투자 망설여지는 게 이거 때문임. 주식시장에 대한 개념이 안 잡혀있음.   \n",
       "\n",
       "                                                   tokens  \\\n",
       "891011  ['미국', '주식', '보다', '한국', '꺼', '보', '면', 'ㄹㅇ', ...   \n",
       "891012  ['한국', '상장', '회사', '들', '중', '에', 'ㅂㅅ', '같', '...   \n",
       "891013  ['신흥국', '회사', '투자', '망설여', '지', '는', '게', '이거'...   \n",
       "\n",
       "                                           tokens_reduced  \n",
       "891011  ['미국', '주식', '보다', '한국', '꺼', '보', '면', 'ㄹㅇ', ...  \n",
       "891012  ['한국', '상장', '회사', '들', '중', '에', 'ㅂㅅ', '같', '...  \n",
       "891013  ['신흥국', '회사', '투자', '<UNK>', '지', '는', '게', '이...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"5_tokenized_text_10000.csv\", encoding=\"utf-8\")\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84be1b2b-49b9-4c84-9a5f-bfef7d7cd64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891014 entries, 0 to 891013\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   search_keyword  891014 non-null  object\n",
      " 1   date_created    891014 non-null  object\n",
      " 2   time_created    891014 non-null  object\n",
      " 3   writer          891014 non-null  object\n",
      " 4   is_reply        891014 non-null  int64 \n",
      " 5   id              891014 non-null  int64 \n",
      " 6   spaced_text     891014 non-null  object\n",
      " 7   tokens          891014 non-null  object\n",
      " 8   tokens_reduced  891014 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 61.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43761cea-5fd4-43ff-bddc-4a27a0755d77",
   "metadata": {},
   "source": [
    "### 6-b-2. 임베딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c81baa1-9c3b-4336-8999-830654c93088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98cd8931-762e-443b-aaea-c7bae96bbed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached h5py-3.10.0-cp38-cp38-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached protobuf-4.25.1-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp38-cp38-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached grpcio-1.60.0-cp38-cp38-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading google_auth-2.26.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\.conda\\envs\\brnn\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.11.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached grpcio-1.60.0-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "Using cached h5py-3.10.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Using cached protobuf-4.25.1-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp38-cp38-win_amd64.whl (37 kB)\n",
      "Downloading google_auth-2.26.1-py2.py3-none-any.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/186.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 174.1/186.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 186.4/186.4 kB 2.3 MB/s eta 0:00:00\n",
      "Using cached Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.26.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.60.0 h5py-3.10.0 keras-2.13.1 libclang-16.0.6 markdown-3.5.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.1 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.1 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.5.2 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.14.5 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b01cb4b-1084-48d8-b2ea-ea5ad3ef5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 776, 50)           5739400   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5739400 (21.89 MB)\n",
      "Trainable params: 5739400 (21.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# 토큰화 및 단어 사전 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tokens'])\n",
    "sequences = tokenizer.texts_to_sequences(df['tokens'])\n",
    "\n",
    "# 패딩 적용\n",
    "max_seq_length = max(len(x) for x in sequences)\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# 단어 사전 크기 및 임베딩 차원 설정\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "# 임베딩 층을 포함하는 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
    "\n",
    "# 모델 요약 출력\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f720f02-50d9-4485-a3ae-53b96a33f714",
   "metadata": {},
   "source": [
    "### 6-b-3. 임베딩 결과 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d93fcaa-b66e-4074-a109-8a003f71a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.conda\\envs\\brnn\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "# 모델 생성 및 컴파일 코드는 여기에 추가합니다.\n",
    "\n",
    "# 모델 구조와 가중치를 파일로 저장\n",
    "model.save('6_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ef47c-8441-4795-afaf-eb70fb46035d",
   "metadata": {},
   "source": [
    "### 6-b-4. 결과 확인 : 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075994ff-ea7f-4662-96f9-d3227888abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# 모델 파일 로드\n",
    "loaded_model = load_model('6_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bf800-9f6e-4723-8982-a7c8737f9ae9",
   "metadata": {},
   "source": [
    "### 6-b-5. 단어 사전 크기, 임베딩 차원 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7bb8e8-9e21-41ae-b7b5-4fa8480af55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114788, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_file_path = '6_embedding_model.h5'\n",
    "\n",
    "# 모델 로드\n",
    "embedding_model = load_model(model_file_path)\n",
    "\n",
    "# 임베딩 레이어의 가중치 확인\n",
    "embedding_weights = embedding_model.layers[0].get_weights()[0]\n",
    "embedding_weights.shape # 임베딩 차원과 단어 사전 크기 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
